{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8b7c1ed436a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;31m# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mfetch_detail_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;31m# 끝나면 브라우져 닫기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-8b7c1ed436a8>\u001b[0m in \u001b[0;36mfetch_detail_url\u001b[1;34m()\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2598\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# 다운받을 폴더경로 입력\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"c:/googleimages/snake/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;31m# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    223\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    529\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             response = self.parent.error(\n\u001b[1;32m--> 641\u001b[1;33m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    650\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from  bs4 import BeautifulSoup\n",
    "from selenium import webdriver  # 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time                     # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 위의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'D:\\chromedriver/chromedriver.exe'\n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# 구글의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url)\n",
    "browser.get(\"https://www.google.co.kr/imghp?hl=ko&tab=wi&ei=l1AdWbegOcra8QXvtr-4Cw&ved=0EKouCBUoAQ\")\n",
    "\n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "#elem = browser.find_elements_by_class_name('gLFyf gsfi')\n",
    "\n",
    "elem = browser.find_element_by_xpath(\"//*[@class='gLFyf gsfi']\") \n",
    "\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"snake python\")\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "for i in range(1, 20):\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)                  # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_=\"rg_i\")  # 구글 이미지 url 이 있는 img 태그의 _img 클래스에 가서\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~ \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,2598):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"c:/googleimages/snake/\" + str(idx) + \".jpg\")\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ElementNotVisibleException",
     "evalue": "Message: element not visible\n  (Session info: chrome=84.0.4147.125)\n  (Driver info: chromedriver=2.41.578737 (49da6702b16031c40d63e5618de03a32ff6c197e),platform=Windows NT 10.0.18363 x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mElementNotVisibleException\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e108d355195f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mbrowser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"//*[@class='mye4qd']\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 여기가 결과 더보기 코드입니다\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 4번 스크롤 내려가게 구현된 상태 range(1,5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36mclick\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;34m\"\"\"Clicks the element.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCommand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCLICK_ELEMENT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msubmit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webelement.py\u001b[0m in \u001b[0;36m_execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mElementNotVisibleException\u001b[0m: Message: element not visible\n  (Session info: chrome=84.0.4147.125)\n  (Driver info: chromedriver=2.41.578737 (49da6702b16031c40d63e5618de03a32ff6c197e),platform=Windows NT 10.0.18363 x86_64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib.request # 웹 url을 파이썬이 인식 할 수 있게하는 패키지\n",
    "from  bs4 import BeautifulSoup # html에서 데이터 검색을 용이하게 하는 패키지\n",
    "from selenium import webdriver  \n",
    "# selenium : 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크 \n",
    "# 손으로 클릭하면서 컴퓨터가 대신하면서 스크롤링하게 하는 패키지\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time       # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 아래 파일경로의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'D:\\chromedriver/chromedriver.exe'\n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# 구글의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url) \n",
    "browser.get(\"https://www.google.co.kr/imghp?hl=ko&tab=wi&ei=l1AdWbegOcra8QXvtr-4Cw&ved=0EKouCBUoAQ\") \n",
    "\n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "# elem = browser.find_elements_by_class_name('gLFyf gsfi') # Tip : f12누른후 커서를 검색창에 올려두고 id를 찾으면 best\n",
    "\n",
    "elem = browser.find_element_by_xpath(\"//*[@class='gLFyf gsfi']\")  # 위의 코드대로 하거나 이렇게 하거나 둘 중 하나 select\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"ocean\") # 여기에 스크롤링하고싶은 검색어를 입력\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "# 현재 결과 더보기는 구현 되어있지 않은상태 -> 구글의 경우 400개 image가 저장됨.\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "\n",
    "for i in range(1, 6): # 5번 스크롤 내려가게 구현된 상태 range(1,5)\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)          # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌 / 키보드 end키를 총 5번 누르는데 end1번누르고 10초 쉼\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌(이거 안하면 하얀색 이미지가 다운받아질 수 있음.)\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "\n",
    "browser.find_element_by_xpath(\"//*[@class='mye4qd']\").click()  # 여기가 결과 더보기 코드입니다\n",
    "\n",
    "for i in range(1, 5): # 4번 스크롤 내려가게 구현된 상태 range(1,5)\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)          # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌 / 키보드 end키를 총 5번 누르는데 end1번누르고 10초 쉼\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌(이거 안하면 하얀색 이미지가 다운받아질 수 있음.)\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "### 검색한 구글 이미지의 url을 따오는 코드 ###\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_=\"rg_i Q4LuWd\")  # 구글 이미지 url 이 있는 img 태그의 _img 클래스에 가서 (f12로 확인가능.)\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음.\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# except부분\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~  \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,1):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"c:/googleimages/ocean1/\"+ str(idx) + \"_google.jpg\")\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "\n",
    "import urllib.request # 웹 url을 파이썬이 인식 할 수 있게하는 패키지\n",
    "from  bs4 import BeautifulSoup # html에서 데이터 검색을 용이하게 하는 패키지\n",
    "from selenium import webdriver  \n",
    "# selenium : 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크 \n",
    "# 손으로 클릭하면서 컴퓨터가 대신하면서 스크롤링하게 하는 패키지\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time       # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 아래 파일경로의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'D:\\chromedriver/chromedriver.exe' \n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# Bing의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url) \n",
    "browser.get(\"https://www.bing.com/images?FORM=Z9LH\")  # bing의 이미지 검색창에 들어가서 url을 가져오면 됨.\n",
    "\n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "# elem = browser.find_elements_by_class_name('b_searchbox') # Tip : f12누른후 커서를 검색창에 올려두고 class이름이나 id 찾아보기.\n",
    "\n",
    "elem = browser.find_element_by_xpath(\"//*[@class='sb_form_q']\")  # 위의 코드대로 하거나 이렇게 하거나 둘 중 하나 select\n",
    "\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"ocean\") # 여기에 스크롤링하고싶은 검색어를 입력\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "# 현재 결과 더보기는 구현 되어있지 않은상태 -> 구글의 경우 400개 image가 저장됨.\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "for i in range(1, 5): # 4번 스크롤 내려가게 구현된 상태 range(1,5)\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)          # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌 / 키보드 end키를 총 5번 누르는데 end1번누르고 10초 쉼\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌(이거 안하면 하얀색 이미지가 다운받아질 수 있음.)\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "### 검색한 bing 이미지의 url을 따오는 코드 ###\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_= \"mimg\")  # bing 이미지 url 이 있는 img 태그의 _img 클래스에 가서 (f12로 확인가능.)\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음.\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# except부분\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~  \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,1):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"c:/googleimages/ocean1/\" + str(idx) + \".jpg\")  # 저장 폴더 위치 바꾸기\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "\n",
    "import urllib.request # 웹 url을 파이썬이 인식 할 수 있게하는 패키지\n",
    "from  bs4 import BeautifulSoup # html에서 데이터 검색을 용이하게 하는 패키지\n",
    "from selenium import webdriver  \n",
    "# selenium : 웹 애플리케이션의 테스트를 자동화하기 위한 프레임 워크 \n",
    "# 손으로 클릭하면서 컴퓨터가 대신하면서 스크롤링하게 하는 패키지\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time       # 중간중간 sleep 을 걸어야 해서 time 모듈 import\n",
    "\n",
    "########################### url 받아오기 ###########################\n",
    "\n",
    "# 웹브라우져로 크롬을 사용할거라서 크롬 드라이버를 다운받아 아래 파일경로의 위치에 둔다\n",
    "# 팬텀 js로 하면 백그라운드로 실행할 수 있음\n",
    "binary = 'D:\\chromedriver/chromedriver.exe' \n",
    "\n",
    "# 브라우져를 인스턴스화\n",
    "browser = webdriver.Chrome(binary)\n",
    "\n",
    "# naver의 이미지 검색 url 받아옴(아무것도 안 쳤을때의 url) \n",
    "browser.get(\"https://search.naver.com/search.naver?where=image&amp;sm=stb_nmr&amp;\")  \n",
    "# 구글의 이미지 검색에 해당하는 input 창의 id 가 '  ?  ' 임(검색창에 해당하는 html코드를 찾아서 elem 사용하도록 설정)\n",
    "# input창 찾는 방법은 원노트에 있음\n",
    "\n",
    "elem = browser.find_element_by_id(\"nx_query\") # Tip : f12누른후 커서를 검색창에 올려두고 class이름이나 id 찾아보기.\n",
    "                                               # ID를 입력할때는 이렇게 by_class_name 을 지우고 by_id입력.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### 검색어 입력 ###########################\n",
    "\n",
    "# elem 이 input 창과 연결되어 스스로 햄버거를 검색\n",
    "elem.send_keys(\"\") # 여기에 스크롤링하고싶은 검색어를 입력\n",
    "\n",
    "# 웹에서의 submit 은 엔터의 역할을 함\n",
    "elem.submit()\n",
    "\n",
    "# 현재 결과 더보기는 구현 되어있지 않은상태 -> 구글의 경우 400개 image가 저장됨.\n",
    "\n",
    "########################### 반복할 횟수 ###########################\n",
    "\n",
    "# 스크롤을 내리려면 브라우져 이미지 검색결과 부분(바디부분)에 마우스 클릭 한번 하고 End키를 눌러야함\n",
    "for i in range(1, 4): # 5번 스크롤 내려가게 구현된 상태 range(1,5)\n",
    "    browser.find_element_by_xpath(\"//body\").send_keys(Keys.END)\n",
    "    time.sleep(10)          # END 키 누르고 내려가는데 시간이 걸려서 sleep 해줌 / 키보드 end키를 총 5번 누르는데 end1번누르고 10초 쉼\n",
    "\n",
    "time.sleep(10)                      # 네트워크 느릴까봐 안정성 위해 sleep 해줌(이거 안하면 하얀색 이미지가 다운받아질 수 있음.)\n",
    "html = browser.page_source         # 크롬브라우져에서 현재 불러온 소스 가져옴\n",
    "soup = BeautifulSoup(html, \"lxml\") # html 코드를 검색할 수 있도록 설정\n",
    "\n",
    "\n",
    "\n",
    "########################### 그림파일 저장 ###########################\n",
    "\n",
    "### 검색한 네이버 이미지의 url을 따오는 코드 ###\n",
    "def fetch_list_url():\n",
    "    params = []\n",
    "    imgList = soup.find_all(\"img\", class_= \"_img\")  # naver 이미지 url 이 있는 img 태그의 _img 클래스에 가서 (f12로 확인가능.)\n",
    "    for im in imgList:\n",
    "        try :\n",
    "            params.append(im[\"src\"])                   # params 리스트에 image url 을 담음.\n",
    "        except KeyError:\n",
    "            params.append(im[\"data-src\"])\n",
    "    return params\n",
    "\n",
    "# except부분\n",
    "# 이미지의 상세 url 의 값이 있는 src 가 없을 경우\n",
    "# data-src 로 가져오시오 ~  \n",
    "\n",
    "\n",
    "def fetch_detail_url():\n",
    "    params = fetch_list_url()\n",
    "\n",
    "    for idx,p in enumerate(params,1):\n",
    "        # 다운받을 폴더경로 입력\n",
    "        urllib.request.urlretrieve(p, \"c:/googleimages/ocean1/\" + str(idx) + \".jpg\")  # 저장 폴더 위치 바꾸기\n",
    "\n",
    "# enumerate 는 리스트의 모든 요소를 인덱스와 쌍으로 추출\n",
    "# 하는 함수 . 숫자 1은 인덱스를 1부터 시작해라 ~\n",
    "\n",
    "fetch_detail_url()\n",
    "\n",
    "# 끝나면 브라우져 닫기\n",
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
